% \mypar{Motivation}
% What is online dictionary learning, why is this important.
% mention we only optimize lars?
% We optimize LARS, why we optimize LARS
% What have we optimized?
% One sentence result on how we improve it.
% Give a small description on what each section is doing
% \mypar{Related work?}


% \mypar{Online Sparse Dictionary Learning}
% % include the psuedo-code from the original paper?

Least Angle Regression (LARS) is a well-known sparse coding algorithm proposed by Bardley Efron et al\cite{Efron:04}. The algorithm is known for being less greedy and more computationally efficient to its ancestors, Forward Selection\lc{cite} and Forward Stagewise\lc{cite}.
One of the most common application of LARS is features selection.
Often, while training on really large dataset, the number of features within a data significantly increase the time to train the model. To decrease the number of features that represent a data while preserving the characteristics, the algorithm that is used to select the remaining features is important.

Dictionary learning, widely used in machine learning, signal processing, and image processing, is a typical case that uses feature selection algorithms.
The goal of dictionary learning is to approximate a given signal using only a few basis elements from the learned dictionary. More specifically, given a bunch of signals $Y$ in $\mathbb{R}^d$ and a dictionary $X$ in $\mathbb{R}^{d \times k}$, with k columns referred to as atoms, the algorithm is meant for finding a linear combination of most representative atoms from the dictionary.

To handle very large training sets and dynamic training data changing over time, Mairal et al. proposes an online learning algorithm for dictionary learning\cite{Mairal:2010}. In each iteration $i$ of the algorithm, sparse coding is used to compute the sparse representation $\beta_i$ of a random sample $y$ in $\mathbb{R}^d$ from a distribution $p(y)$. $\beta_i$ is then used to update the dictionary $X$.

% \mypar{Least Angle Regression (LARS)}
While feature selection algorithm like LARS are implemented to decrease the amount of time needed for training, it is obvious that the overhead of applying it should be as little as possible.
And therefore, the goal of this paper is to proposed optimization methods that are applicable to implementations of LARS and its variants, such as LASSO\lc{cite}, and justify these optimization methods do speed up the entire procedure and enhance the performance.

In the following sections, we formally define LARS, introduce incremental Cholesky decomposition and shows a detail cost analysis on the entire procedure in Section~\ref{sec:background}.
We present the optimization we applied in Section~\ref{sec:method} and show the result in Section~\ref{sec:experiment}. At last, we summarize our work in Section~\ref{sec:conclusions}.






%  Do not start the introduction with the abstract or a slightly modified
%  version. It follows a possible structure of the introduction. 
%  Note that the structure can be modified, but the
%  content should be the same. Introduction and abstract should fill at most the first page, better less.
 
%  \mypar{Motivation} The first task is to motivate what you do.  You can
%  start general and zoom in one the specific problem you consider.  In
%  the process you should have explained to the reader: what you are doing,
%  why you are doing, why it is important (order is usually reversed).
 
%  For example, if my result is the fastest DFT implementation ever, one
%  could roughly go as follows. First explain why the DFT is important
%  (used everywhere with a few examples) and why performance matters (large datasets,
%  realtime). Then explain that fast implementations are very hard and
%  expensive to get (memory hierarchy, vector, parallel). 
 
%  Now you state what you do in this paper. In our example: 
%  presenting a DFT implementation that is
%  faster for some sizes than all the other ones.
 
%  \mypar{Related work} Next, you have to give a brief overview of
%  related work. For a paper like this, anywhere between 2 and 8
%  references. Briefly explain what they do. In the end contrast to what
%  you do to make now precisely clear what your contribution is.
